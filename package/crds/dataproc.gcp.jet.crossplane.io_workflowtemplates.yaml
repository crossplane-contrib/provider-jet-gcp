apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.6.2
  creationTimestamp: null
  name: workflowtemplates.dataproc.gcp.jet.crossplane.io
spec:
  group: dataproc.gcp.jet.crossplane.io
  names:
    categories:
    - crossplane
    - managed
    - gcpjet
    kind: WorkflowTemplate
    listKind: WorkflowTemplateList
    plural: workflowtemplates
    singular: workflowtemplate
  scope: Cluster
  versions:
  - additionalPrinterColumns:
    - jsonPath: .status.conditions[?(@.type=='Ready')].status
      name: READY
      type: string
    - jsonPath: .status.conditions[?(@.type=='Synced')].status
      name: SYNCED
      type: string
    - jsonPath: .metadata.annotations.crossplane\.io/external-name
      name: EXTERNAL-NAME
      type: string
    - jsonPath: .metadata.creationTimestamp
      name: AGE
      type: date
    name: v1alpha1
    schema:
      openAPIV3Schema:
        description: WorkflowTemplate is the Schema for the WorkflowTemplates API
        properties:
          apiVersion:
            description: 'APIVersion defines the versioned schema of this representation
              of an object. Servers should convert recognized schemas to the latest
              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
            type: string
          kind:
            description: 'Kind is a string value representing the REST resource this
              object represents. Servers may infer this from the endpoint the client
              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
            type: string
          metadata:
            type: object
          spec:
            description: WorkflowTemplateSpec defines the desired state of WorkflowTemplate
            properties:
              deletionPolicy:
                default: Delete
                description: DeletionPolicy specifies what will happen to the underlying
                  external when this managed resource is deleted - either "Delete"
                  or "Orphan" the external resource.
                enum:
                - Orphan
                - Delete
                type: string
              forProvider:
                properties:
                  dagTimeout:
                    description: Optional. Timeout duration for the DAG of jobs, expressed
                      in seconds (see [JSON representation of duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
                      The timeout duration must be from 10 minutes ("600s") to 24
                      hours ("86400s"). The timer begins when the first job is submitted.
                      If the workflow is running at the end of the timeout period,
                      any remaining jobs are cancelled, the workflow is ended, and
                      if the workflow was running on a [managed cluster](/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster),
                      the cluster is deleted.
                    type: string
                  jobs:
                    description: Required. The Directed Acyclic Graph of Jobs to submit.
                    items:
                      properties:
                        hadoopJob:
                          description: Optional. Job is a Hadoop job.
                          items:
                            properties:
                              archiveUris:
                                description: 'Optional. HCFS URIs of archives to be
                                  extracted in the working directory of Hadoop drivers
                                  and tasks. Supported file types: .jar, .tar, .tar.gz,
                                  .tgz, or .zip.'
                                items:
                                  type: string
                                type: array
                              args:
                                description: Optional. The arguments to pass to the
                                  driver. Do not include arguments, such as `-libjars`
                                  or `-Dfoo=bar`, that can be set as job properties,
                                  since a collision may occur that causes an incorrect
                                  job submission.
                                items:
                                  type: string
                                type: array
                              fileUris:
                                description: Optional. HCFS (Hadoop Compatible Filesystem)
                                  URIs of files to be copied to the working directory
                                  of Hadoop drivers and distributed tasks. Useful
                                  for naively parallel tasks.
                                items:
                                  type: string
                                type: array
                              jarFileUris:
                                description: Optional. Jar file URIs to add to the
                                  CLASSPATHs of the Hadoop driver and tasks.
                                items:
                                  type: string
                                type: array
                              loggingConfig:
                                description: Optional. The runtime log config for
                                  job execution.
                                items:
                                  properties:
                                    driverLogLevels:
                                      additionalProperties:
                                        type: string
                                      description: 'The per-package log levels for
                                        the driver. This may include "root" package
                                        name to configure rootLogger. Examples: ''com.google
                                        = FATAL'', ''root = INFO'', ''org.apache =
                                        DEBUG'''
                                      type: object
                                  type: object
                                type: array
                              mainClass:
                                description: The name of the driver's main class.
                                  The jar file containing the class must be in the
                                  default CLASSPATH or specified in `jar_file_uris`.
                                type: string
                              mainJarFileUri:
                                description: 'The HCFS URI of the jar file containing
                                  the main class. Examples: ''gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar''
                                  ''hdfs:/tmp/test-samples/custom-wordcount.jar''
                                  ''file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'''
                                type: string
                              properties:
                                additionalProperties:
                                  type: string
                                description: Optional. A mapping of property names
                                  to values, used to configure Hadoop. Properties
                                  that conflict with values set by the Dataproc API
                                  may be overwritten. Can include properties set in
                                  /etc/hadoop/conf/*-site and classes in user code.
                                type: object
                            type: object
                          type: array
                        hiveJob:
                          description: Optional. Job is a Hive job.
                          items:
                            properties:
                              continueOnFailure:
                                description: Optional. Whether to continue executing
                                  queries if a query fails. The default value is `false`.
                                  Setting to `true` can be useful when executing independent
                                  parallel queries.
                                type: boolean
                              jarFileUris:
                                description: Optional. HCFS URIs of jar files to add
                                  to the CLASSPATH of the Hive server and Hadoop MapReduce
                                  (MR) tasks. Can contain Hive SerDes and UDFs.
                                items:
                                  type: string
                                type: array
                              properties:
                                additionalProperties:
                                  type: string
                                description: Optional. A mapping of property names
                                  and values, used to configure Hive. Properties that
                                  conflict with values set by the Dataproc API may
                                  be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml,
                                  /etc/hive/conf/hive-site.xml, and classes in user
                                  code.
                                type: object
                              queryFileUri:
                                description: The HCFS URI of the script that contains
                                  Hive queries.
                                type: string
                              queryList:
                                description: A list of queries.
                                items:
                                  properties:
                                    queries:
                                      description: 'Required. The queries to execute.
                                        You do not need to end a query expression
                                        with a semicolon. Multiple queries can be
                                        specified in one string by separating each
                                        with a semicolon. Here is an example of a
                                        Dataproc API snippet that uses a QueryList
                                        to specify a HiveJob: "hiveJob": { "queryList":
                                        { "queries": [ "query1", "query2", "query3;query4",
                                        ] } }'
                                      items:
                                        type: string
                                      type: array
                                  required:
                                  - queries
                                  type: object
                                type: array
                              scriptVariables:
                                additionalProperties:
                                  type: string
                                description: 'Optional. Mapping of query variable
                                  names to values (equivalent to the Hive command:
                                  `SET name="value";`).'
                                type: object
                            type: object
                          type: array
                        labels:
                          additionalProperties:
                            type: string
                          description: 'Optional. The labels to associate with this
                            job. Label keys must be between 1 and 63 characters long,
                            and must conform to the following regular expression:
                            p{Ll}p{Lo}{0,62} Label values must be between 1 and 63
                            characters long, and must conform to the following regular
                            expression: [p{Ll}p{Lo}p{N}_-]{0,63} No more than 32 labels
                            can be associated with a given job.'
                          type: object
                        pigJob:
                          description: Optional. Job is a Pig job.
                          items:
                            properties:
                              continueOnFailure:
                                description: Optional. Whether to continue executing
                                  queries if a query fails. The default value is `false`.
                                  Setting to `true` can be useful when executing independent
                                  parallel queries.
                                type: boolean
                              jarFileUris:
                                description: Optional. HCFS URIs of jar files to add
                                  to the CLASSPATH of the Pig Client and Hadoop MapReduce
                                  (MR) tasks. Can contain Pig UDFs.
                                items:
                                  type: string
                                type: array
                              loggingConfig:
                                description: Optional. The runtime log config for
                                  job execution.
                                items:
                                  properties:
                                    driverLogLevels:
                                      additionalProperties:
                                        type: string
                                      description: 'The per-package log levels for
                                        the driver. This may include "root" package
                                        name to configure rootLogger. Examples: ''com.google
                                        = FATAL'', ''root = INFO'', ''org.apache =
                                        DEBUG'''
                                      type: object
                                  type: object
                                type: array
                              properties:
                                additionalProperties:
                                  type: string
                                description: Optional. A mapping of property names
                                  to values, used to configure Pig. Properties that
                                  conflict with values set by the Dataproc API may
                                  be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml,
                                  /etc/pig/conf/pig.properties, and classes in user
                                  code.
                                type: object
                              queryFileUri:
                                description: The HCFS URI of the script that contains
                                  the Pig queries.
                                type: string
                              queryList:
                                description: A list of queries.
                                items:
                                  properties:
                                    queries:
                                      description: 'Required. The queries to execute.
                                        You do not need to end a query expression
                                        with a semicolon. Multiple queries can be
                                        specified in one string by separating each
                                        with a semicolon. Here is an example of a
                                        Dataproc API snippet that uses a QueryList
                                        to specify a HiveJob: "hiveJob": { "queryList":
                                        { "queries": [ "query1", "query2", "query3;query4",
                                        ] } }'
                                      items:
                                        type: string
                                      type: array
                                  required:
                                  - queries
                                  type: object
                                type: array
                              scriptVariables:
                                additionalProperties:
                                  type: string
                                description: 'Optional. Mapping of query variable
                                  names to values (equivalent to the Pig command:
                                  `name=[value]`).'
                                type: object
                            type: object
                          type: array
                        prerequisiteStepIds:
                          description: Optional. The optional list of prerequisite
                            job step_ids. If not specified, the job will start at
                            the beginning of workflow.
                          items:
                            type: string
                          type: array
                        prestoJob:
                          description: Optional. Job is a Presto job.
                          items:
                            properties:
                              clientTags:
                                description: Optional. Presto client tags to attach
                                  to this query
                                items:
                                  type: string
                                type: array
                              continueOnFailure:
                                description: Optional. Whether to continue executing
                                  queries if a query fails. The default value is `false`.
                                  Setting to `true` can be useful when executing independent
                                  parallel queries.
                                type: boolean
                              loggingConfig:
                                description: Optional. The runtime log config for
                                  job execution.
                                items:
                                  properties:
                                    driverLogLevels:
                                      additionalProperties:
                                        type: string
                                      description: 'The per-package log levels for
                                        the driver. This may include "root" package
                                        name to configure rootLogger. Examples: ''com.google
                                        = FATAL'', ''root = INFO'', ''org.apache =
                                        DEBUG'''
                                      type: object
                                  type: object
                                type: array
                              outputFormat:
                                description: Optional. The format in which query output
                                  will be displayed. See the Presto documentation
                                  for supported output formats
                                type: string
                              properties:
                                additionalProperties:
                                  type: string
                                description: Optional. A mapping of property names
                                  to values. Used to set Presto [session properties](https://prestodb.io/docs/current/sql/set-session.html)
                                  Equivalent to using the --session flag in the Presto
                                  CLI
                                type: object
                              queryFileUri:
                                description: The HCFS URI of the script that contains
                                  SQL queries.
                                type: string
                              queryList:
                                description: A list of queries.
                                items:
                                  properties:
                                    queries:
                                      description: 'Required. The queries to execute.
                                        You do not need to end a query expression
                                        with a semicolon. Multiple queries can be
                                        specified in one string by separating each
                                        with a semicolon. Here is an example of a
                                        Dataproc API snippet that uses a QueryList
                                        to specify a HiveJob: "hiveJob": { "queryList":
                                        { "queries": [ "query1", "query2", "query3;query4",
                                        ] } }'
                                      items:
                                        type: string
                                      type: array
                                  required:
                                  - queries
                                  type: object
                                type: array
                            type: object
                          type: array
                        pysparkJob:
                          description: Optional. Job is a PySpark job.
                          items:
                            properties:
                              archiveUris:
                                description: 'Optional. HCFS URIs of archives to be
                                  extracted into the working directory of each executor.
                                  Supported file types: .jar, .tar, .tar.gz, .tgz,
                                  and .zip.'
                                items:
                                  type: string
                                type: array
                              args:
                                description: Optional. The arguments to pass to the
                                  driver. Do not include arguments, such as `--conf`,
                                  that can be set as job properties, since a collision
                                  may occur that causes an incorrect job submission.
                                items:
                                  type: string
                                type: array
                              fileUris:
                                description: Optional. HCFS URIs of files to be placed
                                  in the working directory of each executor. Useful
                                  for naively parallel tasks.
                                items:
                                  type: string
                                type: array
                              jarFileUris:
                                description: Optional. HCFS URIs of jar files to add
                                  to the CLASSPATHs of the Python driver and tasks.
                                items:
                                  type: string
                                type: array
                              loggingConfig:
                                description: Optional. The runtime log config for
                                  job execution.
                                items:
                                  properties:
                                    driverLogLevels:
                                      additionalProperties:
                                        type: string
                                      description: 'The per-package log levels for
                                        the driver. This may include "root" package
                                        name to configure rootLogger. Examples: ''com.google
                                        = FATAL'', ''root = INFO'', ''org.apache =
                                        DEBUG'''
                                      type: object
                                  type: object
                                type: array
                              mainPythonFileUri:
                                description: Required. The HCFS URI of the main Python
                                  file to use as the driver. Must be a .py file.
                                type: string
                              properties:
                                additionalProperties:
                                  type: string
                                description: Optional. A mapping of property names
                                  to values, used to configure PySpark. Properties
                                  that conflict with values set by the Dataproc API
                                  may be overwritten. Can include properties set in
                                  /etc/spark/conf/spark-defaults.conf and classes
                                  in user code.
                                type: object
                              pythonFileUris:
                                description: 'Optional. HCFS file URIs of Python files
                                  to pass to the PySpark framework. Supported file
                                  types: .py, .egg, and .zip.'
                                items:
                                  type: string
                                type: array
                            required:
                            - mainPythonFileUri
                            type: object
                          type: array
                        scheduling:
                          description: Optional. Job scheduling configuration.
                          items:
                            properties:
                              maxFailuresPerHour:
                                description: Optional. Maximum number of times per
                                  hour a driver may be restarted as a result of driver
                                  exiting with non-zero code before job is reported
                                  failed. A job may be reported as thrashing if driver
                                  exits with non-zero code 4 times within 10 minute
                                  window. Maximum value is 10.
                                format: int64
                                type: integer
                              maxFailuresTotal:
                                description: Optional. Maximum number of times in
                                  total a driver may be restarted as a result of driver
                                  exiting with non-zero code before job is reported
                                  failed. Maximum value is 240.
                                format: int64
                                type: integer
                            type: object
                          type: array
                        sparkJob:
                          description: Optional. Job is a Spark job.
                          items:
                            properties:
                              archiveUris:
                                description: 'Optional. HCFS URIs of archives to be
                                  extracted into the working directory of each executor.
                                  Supported file types: .jar, .tar, .tar.gz, .tgz,
                                  and .zip.'
                                items:
                                  type: string
                                type: array
                              args:
                                description: Optional. The arguments to pass to the
                                  driver. Do not include arguments, such as `--conf`,
                                  that can be set as job properties, since a collision
                                  may occur that causes an incorrect job submission.
                                items:
                                  type: string
                                type: array
                              fileUris:
                                description: Optional. HCFS URIs of files to be placed
                                  in the working directory of each executor. Useful
                                  for naively parallel tasks.
                                items:
                                  type: string
                                type: array
                              jarFileUris:
                                description: Optional. HCFS URIs of jar files to add
                                  to the CLASSPATHs of the Spark driver and tasks.
                                items:
                                  type: string
                                type: array
                              loggingConfig:
                                description: Optional. The runtime log config for
                                  job execution.
                                items:
                                  properties:
                                    driverLogLevels:
                                      additionalProperties:
                                        type: string
                                      description: 'The per-package log levels for
                                        the driver. This may include "root" package
                                        name to configure rootLogger. Examples: ''com.google
                                        = FATAL'', ''root = INFO'', ''org.apache =
                                        DEBUG'''
                                      type: object
                                  type: object
                                type: array
                              mainClass:
                                description: The name of the driver's main class.
                                  The jar file that contains the class must be in
                                  the default CLASSPATH or specified in `jar_file_uris`.
                                type: string
                              mainJarFileUri:
                                description: The HCFS URI of the jar file that contains
                                  the main class.
                                type: string
                              properties:
                                additionalProperties:
                                  type: string
                                description: Optional. A mapping of property names
                                  to values, used to configure Spark. Properties that
                                  conflict with values set by the Dataproc API may
                                  be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf
                                  and classes in user code.
                                type: object
                            type: object
                          type: array
                        sparkRJob:
                          description: Optional. Job is a SparkR job.
                          items:
                            properties:
                              archiveUris:
                                description: 'Optional. HCFS URIs of archives to be
                                  extracted into the working directory of each executor.
                                  Supported file types: .jar, .tar, .tar.gz, .tgz,
                                  and .zip.'
                                items:
                                  type: string
                                type: array
                              args:
                                description: Optional. The arguments to pass to the
                                  driver. Do not include arguments, such as `--conf`,
                                  that can be set as job properties, since a collision
                                  may occur that causes an incorrect job submission.
                                items:
                                  type: string
                                type: array
                              fileUris:
                                description: Optional. HCFS URIs of files to be placed
                                  in the working directory of each executor. Useful
                                  for naively parallel tasks.
                                items:
                                  type: string
                                type: array
                              loggingConfig:
                                description: Optional. The runtime log config for
                                  job execution.
                                items:
                                  properties:
                                    driverLogLevels:
                                      additionalProperties:
                                        type: string
                                      description: 'The per-package log levels for
                                        the driver. This may include "root" package
                                        name to configure rootLogger. Examples: ''com.google
                                        = FATAL'', ''root = INFO'', ''org.apache =
                                        DEBUG'''
                                      type: object
                                  type: object
                                type: array
                              mainRFileUri:
                                description: Required. The HCFS URI of the main R
                                  file to use as the driver. Must be a .R file.
                                type: string
                              properties:
                                additionalProperties:
                                  type: string
                                description: Optional. A mapping of property names
                                  to values, used to configure SparkR. Properties
                                  that conflict with values set by the Dataproc API
                                  may be overwritten. Can include properties set in
                                  /etc/spark/conf/spark-defaults.conf and classes
                                  in user code.
                                type: object
                            required:
                            - mainRFileUri
                            type: object
                          type: array
                        sparkSqlJob:
                          description: Optional. Job is a SparkSql job.
                          items:
                            properties:
                              jarFileUris:
                                description: Optional. HCFS URIs of jar files to be
                                  added to the Spark CLASSPATH.
                                items:
                                  type: string
                                type: array
                              loggingConfig:
                                description: Optional. The runtime log config for
                                  job execution.
                                items:
                                  properties:
                                    driverLogLevels:
                                      additionalProperties:
                                        type: string
                                      description: 'The per-package log levels for
                                        the driver. This may include "root" package
                                        name to configure rootLogger. Examples: ''com.google
                                        = FATAL'', ''root = INFO'', ''org.apache =
                                        DEBUG'''
                                      type: object
                                  type: object
                                type: array
                              properties:
                                additionalProperties:
                                  type: string
                                description: Optional. A mapping of property names
                                  to values, used to configure Spark SQL's SparkConf.
                                  Properties that conflict with values set by the
                                  Dataproc API may be overwritten.
                                type: object
                              queryFileUri:
                                description: The HCFS URI of the script that contains
                                  SQL queries.
                                type: string
                              queryList:
                                description: A list of queries.
                                items:
                                  properties:
                                    queries:
                                      description: 'Required. The queries to execute.
                                        You do not need to end a query expression
                                        with a semicolon. Multiple queries can be
                                        specified in one string by separating each
                                        with a semicolon. Here is an example of a
                                        Dataproc API snippet that uses a QueryList
                                        to specify a HiveJob: "hiveJob": { "queryList":
                                        { "queries": [ "query1", "query2", "query3;query4",
                                        ] } }'
                                      items:
                                        type: string
                                      type: array
                                  required:
                                  - queries
                                  type: object
                                type: array
                              scriptVariables:
                                additionalProperties:
                                  type: string
                                description: 'Optional. Mapping of query variable
                                  names to values (equivalent to the Spark SQL command:
                                  SET `name="value";`).'
                                type: object
                            type: object
                          type: array
                        stepId:
                          description: Required. The step id. The id must be unique
                            among all jobs within the template. The step id is used
                            as prefix for job id, as job `goog-dataproc-workflow-step-id`
                            label, and in prerequisiteStepIds field from other steps.
                            The id must contain only letters (a-z, A-Z), numbers (0-9),
                            underscores (_), and hyphens (-). Cannot begin or end
                            with underscore or hyphen. Must consist of between 3 and
                            50 characters.
                          type: string
                      required:
                      - stepId
                      type: object
                    type: array
                  labels:
                    additionalProperties:
                      type: string
                    description: Optional. The labels to associate with this template.
                      These labels will be propagated to all jobs and clusters created
                      by the workflow instance. Label **keys** must contain 1 to 63
                      characters, and must conform to [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
                      Label **values** may be empty, but, if present, must contain
                      1 to 63 characters, and must conform to [RFC 1035](https://www.ietf.org/rfc/rfc1035.txt).
                      No more than 32 labels can be associated with a template.
                    type: object
                  location:
                    description: The location for the resource
                    type: string
                  name:
                    description: 'Output only. The resource name of the workflow template,
                      as described in https://cloud.google.com/apis/design/resource_names.
                      * For `projects.regions.workflowTemplates`, the resource name
                      of the template has the following format: `projects/{project_id}/regions/{region}/workflowTemplates/{template_id}`
                      * For `projects.locations.workflowTemplates`, the resource name
                      of the template has the following format: `projects/{project_id}/locations/{location}/workflowTemplates/{template_id}`'
                    type: string
                  parameters:
                    description: Optional. Template parameters whose values are substituted
                      into the template. Values for parameters must be provided when
                      the template is instantiated.
                    items:
                      properties:
                        description:
                          description: Optional. Brief description of the parameter.
                            Must not exceed 1024 characters.
                          type: string
                        fields:
                          description: 'Required. Paths to all fields that the parameter
                            replaces. A field is allowed to appear in at most one
                            parameter''s list of field paths. A field path is similar
                            in syntax to a google.protobuf.FieldMask. For example,
                            a field path that references the zone field of a workflow
                            template''s cluster selector would be specified as `placement.clusterSelector.zone`.
                            Also, field paths can reference fields using the following
                            syntax: * Values in maps can be referenced by key: * labels[''key'']
                            * placement.clusterSelector.clusterLabels[''key''] * placement.managedCluster.labels[''key'']
                            * placement.clusterSelector.clusterLabels[''key''] * jobs[''step-id''].labels[''key'']
                            * Jobs in the jobs list can be referenced by step-id:
                            * jobs[''step-id''].hadoopJob.mainJarFileUri * jobs[''step-id''].hiveJob.queryFileUri
                            * jobs[''step-id''].pySparkJob.mainPythonFileUri * jobs[''step-id''].hadoopJob.jarFileUris[0]
                            * jobs[''step-id''].hadoopJob.archiveUris[0] * jobs[''step-id''].hadoopJob.fileUris[0]
                            * jobs[''step-id''].pySparkJob.pythonFileUris[0] * Items
                            in repeated fields can be referenced by a zero-based index:
                            * jobs[''step-id''].sparkJob.args[0] * Other examples:
                            * jobs[''step-id''].hadoopJob.properties[''key''] * jobs[''step-id''].hadoopJob.args[0]
                            * jobs[''step-id''].hiveJob.scriptVariables[''key''] *
                            jobs[''step-id''].hadoopJob.mainJarFileUri * placement.clusterSelector.zone
                            It may not be possible to parameterize maps and repeated
                            fields in their entirety since only individual map values
                            and individual items in repeated fields can be referenced.
                            For example, the following field paths are invalid: -
                            placement.clusterSelector.clusterLabels - jobs[''step-id''].sparkJob.args'
                          items:
                            type: string
                          type: array
                        name:
                          description: Required. Parameter name. The parameter name
                            is used as the key, and paired with the parameter value,
                            which are passed to the template when the template is
                            instantiated. The name must contain only capital letters
                            (A-Z), numbers (0-9), and underscores (_), and must not
                            start with a number. The maximum length is 40 characters.
                          type: string
                        validation:
                          description: Optional. Validation rules to be applied to
                            this parameter's value.
                          items:
                            properties:
                              regex:
                                description: Validation based on regular expressions.
                                items:
                                  properties:
                                    regexes:
                                      description: Required. RE2 regular expressions
                                        used to validate the parameter's value. The
                                        value must match the regex in its entirety
                                        (substring matches are not sufficient).
                                      items:
                                        type: string
                                      type: array
                                  required:
                                  - regexes
                                  type: object
                                type: array
                              values:
                                description: Validation based on a list of allowed
                                  values.
                                items:
                                  properties:
                                    values:
                                      description: Required. List of allowed values
                                        for the parameter.
                                      items:
                                        type: string
                                      type: array
                                  required:
                                  - values
                                  type: object
                                type: array
                            type: object
                          type: array
                      required:
                      - fields
                      - name
                      type: object
                    type: array
                  placement:
                    description: Required. WorkflowTemplate scheduling information.
                    items:
                      properties:
                        clusterSelector:
                          description: Optional. A selector that chooses target cluster
                            for jobs based on metadata. The selector is evaluated
                            at the time each job is submitted.
                          items:
                            properties:
                              clusterLabels:
                                additionalProperties:
                                  type: string
                                description: Required. The cluster labels. Cluster
                                  must have all labels to match.
                                type: object
                              zone:
                                description: Optional. The zone where workflow process
                                  executes. This parameter does not affect the selection
                                  of the cluster. If unspecified, the zone of the
                                  first cluster matching the selector is used.
                                type: string
                            required:
                            - clusterLabels
                            type: object
                          type: array
                        managedCluster:
                          description: A cluster that is managed by the workflow.
                          items:
                            properties:
                              clusterName:
                                description: Required. The cluster name prefix. A
                                  unique cluster name will be formed by appending
                                  a random suffix. The name must contain only lower-case
                                  letters (a-z), numbers (0-9), and hyphens (-). Must
                                  begin with a letter. Cannot begin or end with hyphen.
                                  Must consist of between 2 and 35 characters.
                                type: string
                              config:
                                description: Required. The cluster configuration.
                                items:
                                  properties:
                                    autoscalingConfig:
                                      description: Optional. Autoscaling config for
                                        the policy associated with the cluster. Cluster
                                        does not autoscale if this field is unset.
                                      items:
                                        properties:
                                          policy:
                                            description: 'Optional. The autoscaling
                                              policy used by the cluster. Only resource
                                              names including projectid and location
                                              (region) are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
                                              * `projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]`
                                              Note that the policy must be in the
                                              same project and Dataproc region.'
                                            type: string
                                        type: object
                                      type: array
                                    encryptionConfig:
                                      description: Optional. Encryption settings for
                                        the cluster.
                                      items:
                                        properties:
                                          gcePdKmsKeyName:
                                            description: Optional. The Cloud KMS key
                                              name to use for PD disk encryption for
                                              all instances in the cluster.
                                            type: string
                                        type: object
                                      type: array
                                    endpointConfig:
                                      description: Optional. Port/endpoint configuration
                                        for this cluster
                                      items:
                                        properties:
                                          enableHttpPortAccess:
                                            description: Optional. If true, enable
                                              http access to specific ports on the
                                              cluster from external sources. Defaults
                                              to false.
                                            type: boolean
                                        type: object
                                      type: array
                                    gceClusterConfig:
                                      description: Optional. The shared Compute Engine
                                        config settings for all instances in a cluster.
                                      items:
                                        properties:
                                          internalIpOnly:
                                            description: Optional. If true, all instances
                                              in the cluster will only have internal
                                              IP addresses. By default, clusters are
                                              not restricted to internal IP addresses,
                                              and will have ephemeral external IP
                                              addresses assigned to each instance.
                                              This `internal_ip_only` restriction
                                              can only be enabled for subnetwork enabled
                                              networks, and all off-cluster dependencies
                                              must be configured to be accessible
                                              without external IP addresses.
                                            type: boolean
                                          metadata:
                                            additionalProperties:
                                              type: string
                                            description: The Compute Engine metadata
                                              entries to add to all instances (see
                                              [Project and instance metadata](https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
                                            type: object
                                          network:
                                            description: 'Optional. The Compute Engine
                                              network to be used for machine communications.
                                              Cannot be specified with subnetwork_uri.
                                              If neither `network_uri` nor `subnetwork_uri`
                                              is specified, the "default" network
                                              of the project is used, if it exists.
                                              Cannot be a "Custom Subnet Network"
                                              (see [Using Subnetworks](https://cloud.google.com/compute/docs/subnetworks)
                                              for more information). A full URL, partial
                                              URI, or short name are valid. Examples:
                                              * `https://www.googleapis.com/compute/v1/projects/[project_id]/regions/global/default`
                                              * `projects/[project_id]/regions/global/default`
                                              * `default`'
                                            type: string
                                          nodeGroupAffinity:
                                            description: Optional. Node Group Affinity
                                              for sole-tenant clusters.
                                            items:
                                              properties:
                                                nodeGroup:
                                                  description: 'Required. The URI
                                                    of a sole-tenant [node group resource](https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups)
                                                    that the cluster will be created
                                                    on. A full URL, partial URI, or
                                                    node group name are valid. Examples:
                                                    * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1`
                                                    * `projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1`
                                                    * `node-group-1`'
                                                  type: string
                                              required:
                                              - nodeGroup
                                              type: object
                                            type: array
                                          privateIpv6GoogleAccess:
                                            description: 'Optional. The type of IPv6
                                              access for a cluster. Possible values:
                                              PRIVATE_IPV6_GOOGLE_ACCESS_UNSPECIFIED,
                                              INHERIT_FROM_SUBNETWORK, OUTBOUND, BIDIRECTIONAL'
                                            type: string
                                          reservationAffinity:
                                            description: Optional. Reservation Affinity
                                              for consuming Zonal reservation.
                                            items:
                                              properties:
                                                consumeReservationType:
                                                  description: 'Optional. Type of
                                                    reservation to consume Possible
                                                    values: TYPE_UNSPECIFIED, NO_RESERVATION,
                                                    ANY_RESERVATION, SPECIFIC_RESERVATION'
                                                  type: string
                                                key:
                                                  description: Optional. Corresponds
                                                    to the label key of reservation
                                                    resource.
                                                  type: string
                                                values:
                                                  description: Optional. Corresponds
                                                    to the label values of reservation
                                                    resource.
                                                  items:
                                                    type: string
                                                  type: array
                                              type: object
                                            type: array
                                          serviceAccount:
                                            description: Optional. The [Dataproc service
                                              account](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc)
                                              (also see [VM Data Plane identity](https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity))
                                              used by Dataproc cluster VM instances
                                              to access Google Cloud Platform services.
                                              If not specified, the [Compute Engine
                                              default service account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account)
                                              is used.
                                            type: string
                                          serviceAccountScopes:
                                            description: 'Optional. The URIs of service
                                              account scopes to be included in Compute
                                              Engine instances. The following base
                                              set of scopes is always included: *
                                              https://www.googleapis.com/auth/cloud.useraccounts.readonly
                                              * https://www.googleapis.com/auth/devstorage.read_write
                                              * https://www.googleapis.com/auth/logging.write
                                              If no scopes are specified, the following
                                              defaults are also provided: * https://www.googleapis.com/auth/bigquery
                                              * https://www.googleapis.com/auth/bigtable.admin.table
                                              * https://www.googleapis.com/auth/bigtable.data
                                              * https://www.googleapis.com/auth/devstorage.full_control'
                                            items:
                                              type: string
                                            type: array
                                          subnetwork:
                                            description: 'Optional. The Compute Engine
                                              subnetwork to be used for machine communications.
                                              Cannot be specified with network_uri.
                                              A full URL, partial URI, or short name
                                              are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/regions/us-east1/subnetworks/sub0`
                                              * `projects/[project_id]/regions/us-east1/subnetworks/sub0`
                                              * `sub0`'
                                            type: string
                                          tags:
                                            description: The Compute Engine tags to
                                              add to all instances (see [Tagging instances](https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
                                            items:
                                              type: string
                                            type: array
                                          zone:
                                            description: 'Optional. The zone where
                                              the Compute Engine cluster will be located.
                                              On a create request, it is required
                                              in the "global" region. If omitted in
                                              a non-global Dataproc region, the service
                                              will pick a zone in the corresponding
                                              Compute Engine region. On a get request,
                                              zone will always be present. A full
                                              URL, partial URI, or short name are
                                              valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone]`
                                              * `projects/[project_id]/zones/[zone]`
                                              * `us-central1-f`'
                                            type: string
                                        type: object
                                      type: array
                                    initializationActions:
                                      description: 'Optional. Commands to execute
                                        on each node after config is completed. By
                                        default, executables are run on master and
                                        all worker nodes. You can test a node''s `role`
                                        metadata to run an executable on a master
                                        or worker node, as shown below using `curl`
                                        (you can also use `wget`): ROLE=$(curl -H
                                        Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role)
                                        if [[ "${ROLE}" == ''Master'' ]]; then ...
                                        master specific actions ... else ... worker
                                        specific actions ... fi'
                                      items:
                                        properties:
                                          executableFile:
                                            description: Required. Cloud Storage URI
                                              of executable file.
                                            type: string
                                          executionTimeout:
                                            description: Optional. Amount of time
                                              executable has to complete. Default
                                              is 10 minutes (see JSON representation
                                              of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
                                              Cluster creation fails with an explanatory
                                              error message (the name of the executable
                                              that caused the error and the exceeded
                                              timeout period) if the executable is
                                              not completed at end of the timeout
                                              period.
                                            type: string
                                        type: object
                                      type: array
                                    lifecycleConfig:
                                      description: Optional. Lifecycle setting for
                                        the cluster.
                                      items:
                                        properties:
                                          autoDeleteTime:
                                            description: Optional. The time when cluster
                                              will be auto-deleted (see JSON representation
                                              of [Timestamp](https://developers.google.com/protocol-buffers/docs/proto3#json)).
                                            type: string
                                          autoDeleteTtl:
                                            description: Optional. The lifetime duration
                                              of cluster. The cluster will be auto-deleted
                                              at the end of this period. Minimum value
                                              is 10 minutes; maximum value is 14 days
                                              (see JSON representation of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
                                            type: string
                                          idleDeleteTtl:
                                            description: Optional. The duration to
                                              keep the cluster alive while idling
                                              (when no jobs are running). Passing
                                              this threshold will cause the cluster
                                              to be deleted. Minimum value is 5 minutes;
                                              maximum value is 14 days (see JSON representation
                                              of [Duration](https://developers.google.com/protocol-buffers/docs/proto3#json)).
                                            type: string
                                        type: object
                                      type: array
                                    masterConfig:
                                      description: Optional. The Compute Engine config
                                        settings for worker instances in a cluster.
                                      items:
                                        properties:
                                          accelerators:
                                            description: Optional. The Compute Engine
                                              accelerator configuration for these
                                              instances.
                                            items:
                                              properties:
                                                acceleratorCount:
                                                  description: The number of the accelerator
                                                    cards of this type exposed to
                                                    this instance.
                                                  format: int64
                                                  type: integer
                                                acceleratorType:
                                                  description: 'Full URL, partial
                                                    URI, or short name of the accelerator
                                                    type resource to expose to this
                                                    instance. See [Compute Engine
                                                    AcceleratorTypes](https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes).
                                                    Examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80`
                                                    * `projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80`
                                                    * `nvidia-tesla-k80` **Auto Zone
                                                    Exception**: If you are using
                                                    the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
                                                    feature, you must use the short
                                                    name of the accelerator type resource,
                                                    for example, `nvidia-tesla-k80`.'
                                                  type: string
                                              type: object
                                            type: array
                                          diskConfig:
                                            description: Optional. Disk option config
                                              settings.
                                            items:
                                              properties:
                                                bootDiskSizeGb:
                                                  description: Optional. Size in GB
                                                    of the boot disk (default is 500GB).
                                                  format: int64
                                                  type: integer
                                                bootDiskType:
                                                  description: 'Optional. Type of
                                                    the boot disk (default is "pd-standard").
                                                    Valid values: "pd-balanced" (Persistent
                                                    Disk Balanced Solid State Drive),
                                                    "pd-ssd" (Persistent Disk Solid
                                                    State Drive), or "pd-standard"
                                                    (Persistent Disk Hard Disk Drive).
                                                    See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).'
                                                  type: string
                                                numLocalSsds:
                                                  description: Optional. Number of
                                                    attached SSDs, from 0 to 4 (default
                                                    is 0). If SSDs are not attached,
                                                    the boot disk is used to store
                                                    runtime logs and [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html)
                                                    data. If one or more SSDs are
                                                    attached, this runtime bulk data
                                                    is spread across them, and the
                                                    boot disk contains only basic
                                                    config and installed binaries.
                                                  format: int64
                                                  type: integer
                                              type: object
                                            type: array
                                          image:
                                            description: 'Optional. The Compute Engine
                                              image resource used for cluster instances.
                                              The URI can represent an image or image
                                              family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]`
                                              * `projects/[project_id]/global/images/[image-id]`
                                              * `image-id` Image family examples.
                                              Dataproc will use the most recent image
                                              from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]`
                                              * `projects/[project_id]/global/images/family/[custom-image-family-name]`
                                              If the URI is unspecified, it will be
                                              inferred from `SoftwareConfig.image_version`
                                              or the system default.'
                                            type: string
                                          machineType:
                                            description: 'Optional. The Compute Engine
                                              machine type used for cluster instances.
                                              A full URL, partial URI, or short name
                                              are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2`
                                              * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2`
                                              * `n1-standard-2` **Auto Zone Exception**:
                                              If you are using the Dataproc [Auto
                                              Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
                                              feature, you must use the short name
                                              of the machine type resource, for example,
                                              `n1-standard-2`.'
                                            type: string
                                          minCpuPlatform:
                                            description: Optional. Specifies the minimum
                                              cpu platform for the Instance Group.
                                              See [Dataproc -> Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
                                            type: string
                                          numInstances:
                                            description: Optional. The number of VM
                                              instances in the instance group. For
                                              [HA cluster](/dataproc/docs/concepts/configuring-clusters/high-availability)
                                              [master_config](#FIELDS.master_config)
                                              groups, **must be set to 3**. For standard
                                              cluster [master_config](#FIELDS.master_config)
                                              groups, **must be set to 1**.
                                            format: int64
                                            type: integer
                                          preemptibility:
                                            description: 'Optional. Specifies the
                                              preemptibility of the instance group.
                                              The default value for master and worker
                                              groups is `NON_PREEMPTIBLE`. This default
                                              cannot be changed. The default value
                                              for secondary instances is `PREEMPTIBLE`.
                                              Possible values: PREEMPTIBILITY_UNSPECIFIED,
                                              NON_PREEMPTIBLE, PREEMPTIBLE'
                                            type: string
                                        type: object
                                      type: array
                                    secondaryWorkerConfig:
                                      description: Optional. The Compute Engine config
                                        settings for worker instances in a cluster.
                                      items:
                                        properties:
                                          accelerators:
                                            description: Optional. The Compute Engine
                                              accelerator configuration for these
                                              instances.
                                            items:
                                              properties:
                                                acceleratorCount:
                                                  description: The number of the accelerator
                                                    cards of this type exposed to
                                                    this instance.
                                                  format: int64
                                                  type: integer
                                                acceleratorType:
                                                  description: 'Full URL, partial
                                                    URI, or short name of the accelerator
                                                    type resource to expose to this
                                                    instance. See [Compute Engine
                                                    AcceleratorTypes](https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes).
                                                    Examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80`
                                                    * `projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80`
                                                    * `nvidia-tesla-k80` **Auto Zone
                                                    Exception**: If you are using
                                                    the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
                                                    feature, you must use the short
                                                    name of the accelerator type resource,
                                                    for example, `nvidia-tesla-k80`.'
                                                  type: string
                                              type: object
                                            type: array
                                          diskConfig:
                                            description: Optional. Disk option config
                                              settings.
                                            items:
                                              properties:
                                                bootDiskSizeGb:
                                                  description: Optional. Size in GB
                                                    of the boot disk (default is 500GB).
                                                  format: int64
                                                  type: integer
                                                bootDiskType:
                                                  description: 'Optional. Type of
                                                    the boot disk (default is "pd-standard").
                                                    Valid values: "pd-balanced" (Persistent
                                                    Disk Balanced Solid State Drive),
                                                    "pd-ssd" (Persistent Disk Solid
                                                    State Drive), or "pd-standard"
                                                    (Persistent Disk Hard Disk Drive).
                                                    See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).'
                                                  type: string
                                                numLocalSsds:
                                                  description: Optional. Number of
                                                    attached SSDs, from 0 to 4 (default
                                                    is 0). If SSDs are not attached,
                                                    the boot disk is used to store
                                                    runtime logs and [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html)
                                                    data. If one or more SSDs are
                                                    attached, this runtime bulk data
                                                    is spread across them, and the
                                                    boot disk contains only basic
                                                    config and installed binaries.
                                                  format: int64
                                                  type: integer
                                              type: object
                                            type: array
                                          image:
                                            description: 'Optional. The Compute Engine
                                              image resource used for cluster instances.
                                              The URI can represent an image or image
                                              family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]`
                                              * `projects/[project_id]/global/images/[image-id]`
                                              * `image-id` Image family examples.
                                              Dataproc will use the most recent image
                                              from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]`
                                              * `projects/[project_id]/global/images/family/[custom-image-family-name]`
                                              If the URI is unspecified, it will be
                                              inferred from `SoftwareConfig.image_version`
                                              or the system default.'
                                            type: string
                                          machineType:
                                            description: 'Optional. The Compute Engine
                                              machine type used for cluster instances.
                                              A full URL, partial URI, or short name
                                              are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2`
                                              * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2`
                                              * `n1-standard-2` **Auto Zone Exception**:
                                              If you are using the Dataproc [Auto
                                              Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
                                              feature, you must use the short name
                                              of the machine type resource, for example,
                                              `n1-standard-2`.'
                                            type: string
                                          minCpuPlatform:
                                            description: Optional. Specifies the minimum
                                              cpu platform for the Instance Group.
                                              See [Dataproc -> Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
                                            type: string
                                          numInstances:
                                            description: Optional. The number of VM
                                              instances in the instance group. For
                                              [HA cluster](/dataproc/docs/concepts/configuring-clusters/high-availability)
                                              [master_config](#FIELDS.master_config)
                                              groups, **must be set to 3**. For standard
                                              cluster [master_config](#FIELDS.master_config)
                                              groups, **must be set to 1**.
                                            format: int64
                                            type: integer
                                          preemptibility:
                                            description: 'Optional. Specifies the
                                              preemptibility of the instance group.
                                              The default value for master and worker
                                              groups is `NON_PREEMPTIBLE`. This default
                                              cannot be changed. The default value
                                              for secondary instances is `PREEMPTIBLE`.
                                              Possible values: PREEMPTIBILITY_UNSPECIFIED,
                                              NON_PREEMPTIBLE, PREEMPTIBLE'
                                            type: string
                                        type: object
                                      type: array
                                    securityConfig:
                                      description: Optional. Security settings for
                                        the cluster.
                                      items:
                                        properties:
                                          kerberosConfig:
                                            description: Optional. Kerberos related
                                              configuration.
                                            items:
                                              properties:
                                                crossRealmTrustAdminServer:
                                                  description: Optional. The admin
                                                    server (IP or hostname) for the
                                                    remote trusted realm in a cross
                                                    realm trust relationship.
                                                  type: string
                                                crossRealmTrustKdc:
                                                  description: Optional. The KDC (IP
                                                    or hostname) for the remote trusted
                                                    realm in a cross realm trust relationship.
                                                  type: string
                                                crossRealmTrustRealm:
                                                  description: Optional. The remote
                                                    realm the Dataproc on-cluster
                                                    KDC will trust, should the user
                                                    enable cross realm trust.
                                                  type: string
                                                crossRealmTrustSharedPassword:
                                                  description: Optional. The Cloud
                                                    Storage URI of a KMS encrypted
                                                    file containing the shared password
                                                    between the on-cluster Kerberos
                                                    realm and the remote trusted realm,
                                                    in a cross realm trust relationship.
                                                  type: string
                                                enableKerberos:
                                                  description: 'Optional. Flag to
                                                    indicate whether to Kerberize
                                                    the cluster (default: false).
                                                    Set this field to true to enable
                                                    Kerberos on a cluster.'
                                                  type: boolean
                                                kdcDbKey:
                                                  description: Optional. The Cloud
                                                    Storage URI of a KMS encrypted
                                                    file containing the master key
                                                    of the KDC database.
                                                  type: string
                                                keyPassword:
                                                  description: Optional. The Cloud
                                                    Storage URI of a KMS encrypted
                                                    file containing the password to
                                                    the user provided key. For the
                                                    self-signed certificate, this
                                                    password is generated by Dataproc.
                                                  type: string
                                                keystore:
                                                  description: Optional. The Cloud
                                                    Storage URI of the keystore file
                                                    used for SSL encryption. If not
                                                    provided, Dataproc will provide
                                                    a self-signed certificate.
                                                  type: string
                                                keystorePassword:
                                                  description: Optional. The Cloud
                                                    Storage URI of a KMS encrypted
                                                    file containing the password to
                                                    the user provided keystore. For
                                                    the self-signed certificate, this
                                                    password is generated by Dataproc.
                                                  type: string
                                                kmsKey:
                                                  description: Optional. The uri of
                                                    the KMS key used to encrypt various
                                                    sensitive files.
                                                  type: string
                                                realm:
                                                  description: Optional. The name
                                                    of the on-cluster Kerberos realm.
                                                    If not specified, the uppercased
                                                    domain of hostnames will be the
                                                    realm.
                                                  type: string
                                                rootPrincipalPassword:
                                                  description: Optional. The Cloud
                                                    Storage URI of a KMS encrypted
                                                    file containing the root principal
                                                    password.
                                                  type: string
                                                tgtLifetimeHours:
                                                  description: Optional. The lifetime
                                                    of the ticket granting ticket,
                                                    in hours. If not specified, or
                                                    user specifies 0, then default
                                                    value 10 will be used.
                                                  format: int64
                                                  type: integer
                                                truststore:
                                                  description: Optional. The Cloud
                                                    Storage URI of the truststore
                                                    file used for SSL encryption.
                                                    If not provided, Dataproc will
                                                    provide a self-signed certificate.
                                                  type: string
                                                truststorePassword:
                                                  description: Optional. The Cloud
                                                    Storage URI of a KMS encrypted
                                                    file containing the password to
                                                    the user provided truststore.
                                                    For the self-signed certificate,
                                                    this password is generated by
                                                    Dataproc.
                                                  type: string
                                              type: object
                                            type: array
                                        type: object
                                      type: array
                                    softwareConfig:
                                      description: Optional. The config settings for
                                        software inside the cluster.
                                      items:
                                        properties:
                                          imageVersion:
                                            description: Optional. The version of
                                              software inside the cluster. It must
                                              be one of the supported [Dataproc Versions](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions),
                                              such as "1.2" (including a subminor
                                              version, such as "1.2.29"), or the ["preview"
                                              version](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions).
                                              If unspecified, it defaults to the latest
                                              Debian version.
                                            type: string
                                          optionalComponents:
                                            description: Optional. The set of components
                                              to activate on the cluster.
                                            items:
                                              type: string
                                            type: array
                                          properties:
                                            additionalProperties:
                                              type: string
                                            description: 'Optional. The properties
                                              to set on daemon config files. Property
                                              keys are specified in `prefix:property`
                                              format, for example `core:hadoop.tmp.dir`.
                                              The following are supported prefixes
                                              and their mappings: * capacity-scheduler:
                                              `capacity-scheduler.xml` * core: `core-site.xml`
                                              * distcp: `distcp-default.xml` * hdfs:
                                              `hdfs-site.xml` * hive: `hive-site.xml`
                                              * mapred: `mapred-site.xml` * pig: `pig.properties`
                                              * spark: `spark-defaults.conf` * yarn:
                                              `yarn-site.xml` For more information,
                                              see [Cluster properties](https://cloud.google.com/dataproc/docs/concepts/cluster-properties).'
                                            type: object
                                        type: object
                                      type: array
                                    stagingBucket:
                                      description: Optional. A Cloud Storage bucket
                                        used to stage job dependencies, config files,
                                        and job driver console output. If you do not
                                        specify a staging bucket, Cloud Dataproc will
                                        determine a Cloud Storage location (US, ASIA,
                                        or EU) for your cluster's staging bucket according
                                        to the Compute Engine zone where your cluster
                                        is deployed, and then create and manage this
                                        project-level, per-location bucket (see [Dataproc
                                        staging bucket](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)).
                                        **This field requires a Cloud Storage bucket
                                        name, not a URI to a Cloud Storage bucket.**
                                      type: string
                                    tempBucket:
                                      description: Optional. A Cloud Storage bucket
                                        used to store ephemeral cluster and jobs data,
                                        such as Spark and MapReduce history files.
                                        If you do not specify a temp bucket, Dataproc
                                        will determine a Cloud Storage location (US,
                                        ASIA, or EU) for your cluster's temp bucket
                                        according to the Compute Engine zone where
                                        your cluster is deployed, and then create
                                        and manage this project-level, per-location
                                        bucket. The default bucket has a TTL of 90
                                        days, but you can use any TTL (or none) if
                                        you specify a bucket. **This field requires
                                        a Cloud Storage bucket name, not a URI to
                                        a Cloud Storage bucket.**
                                      type: string
                                    workerConfig:
                                      description: Optional. The Compute Engine config
                                        settings for worker instances in a cluster.
                                      items:
                                        properties:
                                          accelerators:
                                            description: Optional. The Compute Engine
                                              accelerator configuration for these
                                              instances.
                                            items:
                                              properties:
                                                acceleratorCount:
                                                  description: The number of the accelerator
                                                    cards of this type exposed to
                                                    this instance.
                                                  format: int64
                                                  type: integer
                                                acceleratorType:
                                                  description: 'Full URL, partial
                                                    URI, or short name of the accelerator
                                                    type resource to expose to this
                                                    instance. See [Compute Engine
                                                    AcceleratorTypes](https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes).
                                                    Examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80`
                                                    * `projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80`
                                                    * `nvidia-tesla-k80` **Auto Zone
                                                    Exception**: If you are using
                                                    the Dataproc [Auto Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
                                                    feature, you must use the short
                                                    name of the accelerator type resource,
                                                    for example, `nvidia-tesla-k80`.'
                                                  type: string
                                              type: object
                                            type: array
                                          diskConfig:
                                            description: Optional. Disk option config
                                              settings.
                                            items:
                                              properties:
                                                bootDiskSizeGb:
                                                  description: Optional. Size in GB
                                                    of the boot disk (default is 500GB).
                                                  format: int64
                                                  type: integer
                                                bootDiskType:
                                                  description: 'Optional. Type of
                                                    the boot disk (default is "pd-standard").
                                                    Valid values: "pd-balanced" (Persistent
                                                    Disk Balanced Solid State Drive),
                                                    "pd-ssd" (Persistent Disk Solid
                                                    State Drive), or "pd-standard"
                                                    (Persistent Disk Hard Disk Drive).
                                                    See [Disk types](https://cloud.google.com/compute/docs/disks#disk-types).'
                                                  type: string
                                                numLocalSsds:
                                                  description: Optional. Number of
                                                    attached SSDs, from 0 to 4 (default
                                                    is 0). If SSDs are not attached,
                                                    the boot disk is used to store
                                                    runtime logs and [HDFS](https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html)
                                                    data. If one or more SSDs are
                                                    attached, this runtime bulk data
                                                    is spread across them, and the
                                                    boot disk contains only basic
                                                    config and installed binaries.
                                                  format: int64
                                                  type: integer
                                              type: object
                                            type: array
                                          image:
                                            description: 'Optional. The Compute Engine
                                              image resource used for cluster instances.
                                              The URI can represent an image or image
                                              family. Image examples: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id]`
                                              * `projects/[project_id]/global/images/[image-id]`
                                              * `image-id` Image family examples.
                                              Dataproc will use the most recent image
                                              from the family: * `https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name]`
                                              * `projects/[project_id]/global/images/family/[custom-image-family-name]`
                                              If the URI is unspecified, it will be
                                              inferred from `SoftwareConfig.image_version`
                                              or the system default.'
                                            type: string
                                          machineType:
                                            description: 'Optional. The Compute Engine
                                              machine type used for cluster instances.
                                              A full URL, partial URI, or short name
                                              are valid. Examples: * `https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2`
                                              * `projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2`
                                              * `n1-standard-2` **Auto Zone Exception**:
                                              If you are using the Dataproc [Auto
                                              Zone Placement](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement)
                                              feature, you must use the short name
                                              of the machine type resource, for example,
                                              `n1-standard-2`.'
                                            type: string
                                          minCpuPlatform:
                                            description: Optional. Specifies the minimum
                                              cpu platform for the Instance Group.
                                              See [Dataproc -> Minimum CPU Platform](https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
                                            type: string
                                          numInstances:
                                            description: Optional. The number of VM
                                              instances in the instance group. For
                                              [HA cluster](/dataproc/docs/concepts/configuring-clusters/high-availability)
                                              [master_config](#FIELDS.master_config)
                                              groups, **must be set to 3**. For standard
                                              cluster [master_config](#FIELDS.master_config)
                                              groups, **must be set to 1**.
                                            format: int64
                                            type: integer
                                          preemptibility:
                                            description: 'Optional. Specifies the
                                              preemptibility of the instance group.
                                              The default value for master and worker
                                              groups is `NON_PREEMPTIBLE`. This default
                                              cannot be changed. The default value
                                              for secondary instances is `PREEMPTIBLE`.
                                              Possible values: PREEMPTIBILITY_UNSPECIFIED,
                                              NON_PREEMPTIBLE, PREEMPTIBLE'
                                            type: string
                                        type: object
                                      type: array
                                  type: object
                                type: array
                              labels:
                                additionalProperties:
                                  type: string
                                description: 'Optional. The labels to associate with
                                  this cluster. Label keys must be between 1 and 63
                                  characters long, and must conform to the following
                                  PCRE regular expression: p{Ll}p{Lo}{0,62} Label
                                  values must be between 1 and 63 characters long,
                                  and must conform to the following PCRE regular expression:
                                  [p{Ll}p{Lo}p{N}_-]{0,63} No more than 32 labels
                                  can be associated with a given cluster.'
                                type: object
                            required:
                            - clusterName
                            - config
                            type: object
                          type: array
                      type: object
                    type: array
                  project:
                    description: The project for the resource
                    type: string
                  version:
                    description: Output only. The current version of this workflow
                      template.
                    format: int64
                    type: integer
                required:
                - jobs
                - location
                - name
                - placement
                type: object
              providerConfigRef:
                default:
                  name: default
                description: ProviderConfigReference specifies how the provider that
                  will be used to create, observe, update, and delete this managed
                  resource should be configured.
                properties:
                  name:
                    description: Name of the referenced object.
                    type: string
                required:
                - name
                type: object
              providerRef:
                description: 'ProviderReference specifies the provider that will be
                  used to create, observe, update, and delete this managed resource.
                  Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`'
                properties:
                  name:
                    description: Name of the referenced object.
                    type: string
                required:
                - name
                type: object
              writeConnectionSecretToRef:
                description: WriteConnectionSecretToReference specifies the namespace
                  and name of a Secret to which any connection details for this managed
                  resource should be written. Connection details frequently include
                  the endpoint, username, and password required to connect to the
                  managed resource.
                properties:
                  name:
                    description: Name of the secret.
                    type: string
                  namespace:
                    description: Namespace of the secret.
                    type: string
                required:
                - name
                - namespace
                type: object
            required:
            - forProvider
            type: object
          status:
            description: WorkflowTemplateStatus defines the observed state of WorkflowTemplate.
            properties:
              atProvider:
                properties:
                  createTime:
                    type: string
                  id:
                    type: string
                  updateTime:
                    type: string
                type: object
              conditions:
                description: Conditions of the resource.
                items:
                  description: A Condition that may apply to a resource.
                  properties:
                    lastTransitionTime:
                      description: LastTransitionTime is the last time this condition
                        transitioned from one status to another.
                      format: date-time
                      type: string
                    message:
                      description: A Message containing details about this condition's
                        last transition from one status to another, if any.
                      type: string
                    reason:
                      description: A Reason for this condition's last transition from
                        one status to another.
                      type: string
                    status:
                      description: Status of this condition; is it currently True,
                        False, or Unknown?
                      type: string
                    type:
                      description: Type of this condition. At most one of each condition
                        type may apply to a resource at any point in time.
                      type: string
                  required:
                  - lastTransitionTime
                  - reason
                  - status
                  - type
                  type: object
                type: array
            type: object
        required:
        - spec
        type: object
    served: true
    storage: true
    subresources:
      status: {}
status:
  acceptedNames:
    kind: ""
    plural: ""
  conditions: []
  storedVersions: []
