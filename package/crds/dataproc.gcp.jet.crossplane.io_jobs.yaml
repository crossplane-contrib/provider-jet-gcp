apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.6.2
  creationTimestamp: null
  name: jobs.dataproc.gcp.jet.crossplane.io
spec:
  group: dataproc.gcp.jet.crossplane.io
  names:
    categories:
    - crossplane
    - managed
    - gcpjet
    kind: Job
    listKind: JobList
    plural: jobs
    singular: job
  scope: Cluster
  versions:
  - additionalPrinterColumns:
    - jsonPath: .status.conditions[?(@.type=='Ready')].status
      name: READY
      type: string
    - jsonPath: .status.conditions[?(@.type=='Synced')].status
      name: SYNCED
      type: string
    - jsonPath: .metadata.annotations.crossplane\.io/external-name
      name: EXTERNAL-NAME
      type: string
    - jsonPath: .metadata.creationTimestamp
      name: AGE
      type: date
    name: v1alpha1
    schema:
      openAPIV3Schema:
        description: Job is the Schema for the Jobs API
        properties:
          apiVersion:
            description: 'APIVersion defines the versioned schema of this representation
              of an object. Servers should convert recognized schemas to the latest
              internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources'
            type: string
          kind:
            description: 'Kind is a string value representing the REST resource this
              object represents. Servers may infer this from the endpoint the client
              submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds'
            type: string
          metadata:
            type: object
          spec:
            description: JobSpec defines the desired state of Job
            properties:
              deletionPolicy:
                default: Delete
                description: DeletionPolicy specifies what will happen to the underlying
                  external when this managed resource is deleted - either "Delete"
                  or "Orphan" the external resource.
                enum:
                - Orphan
                - Delete
                type: string
              forProvider:
                properties:
                  forceDelete:
                    description: By default, you can only delete inactive jobs within
                      Dataproc. Setting this to true, and calling destroy, will ensure
                      that the job is first cancelled before issuing the delete.
                    type: boolean
                  hadoopConfig:
                    description: The config of Hadoop job
                    items:
                      properties:
                        archiveUris:
                          description: HCFS URIs of archives to be extracted in the
                            working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
                          items:
                            type: string
                          type: array
                        args:
                          description: The arguments to pass to the driver.
                          items:
                            type: string
                          type: array
                        fileUris:
                          description: HCFS URIs of files to be copied to the working
                            directory of Spark drivers and distributed tasks. Useful
                            for naively parallel tasks.
                          items:
                            type: string
                          type: array
                        jarFileUris:
                          description: HCFS URIs of jar files to add to the CLASSPATHs
                            of the Spark driver and tasks.
                          items:
                            type: string
                          type: array
                        loggingConfig:
                          description: The runtime logging config of the job
                          items:
                            properties:
                              driverLogLevels:
                                additionalProperties:
                                  type: string
                                description: 'Optional. The per-package log levels
                                  for the driver. This may include ''root'' package
                                  name to configure rootLogger. Examples: ''com.google
                                  = FATAL'', ''root = INFO'', ''org.apache = DEBUG''.'
                                type: object
                            required:
                            - driverLogLevels
                            type: object
                          type: array
                        mainClass:
                          description: The class containing the main method of the
                            driver. Must be in a provided jar or jar that is already
                            on the classpath. Conflicts with main_jar_file_uri
                          type: string
                        mainJarFileUri:
                          description: The HCFS URI of jar file containing the driver
                            jar. Conflicts with main_class
                          type: string
                        properties:
                          additionalProperties:
                            type: string
                          description: A mapping of property names to values, used
                            to configure Spark. Properties that conflict with values
                            set by the Cloud Dataproc API may be overwritten. Can
                            include properties set in /etc/spark/conf/spark-defaults.conf
                            and classes in user code.
                          type: object
                      type: object
                    type: array
                  hiveConfig:
                    description: The config of hive job
                    items:
                      properties:
                        continueOnFailure:
                          description: Whether to continue executing queries if a
                            query fails. The default value is false. Setting to true
                            can be useful when executing independent parallel queries.
                            Defaults to false.
                          type: boolean
                        jarFileUris:
                          description: HCFS URIs of jar files to add to the CLASSPATH
                            of the Hive server and Hadoop MapReduce (MR) tasks. Can
                            contain Hive SerDes and UDFs.
                          items:
                            type: string
                          type: array
                        properties:
                          additionalProperties:
                            type: string
                          description: A mapping of property names and values, used
                            to configure Hive. Properties that conflict with values
                            set by the Cloud Dataproc API may be overwritten. Can
                            include properties set in /etc/hadoop/conf/*-site.xml,
                            /etc/hive/conf/hive-site.xml, and classes in user code.
                          type: object
                        queryFileUri:
                          description: HCFS URI of file containing Hive script to
                            execute as the job. Conflicts with query_list
                          type: string
                        queryList:
                          description: The list of Hive queries or statements to execute
                            as part of the job. Conflicts with query_file_uri
                          items:
                            type: string
                          type: array
                        scriptVariables:
                          additionalProperties:
                            type: string
                          description: 'Mapping of query variable names to values
                            (equivalent to the Hive command: SET name="value";).'
                          type: object
                      type: object
                    type: array
                  labels:
                    additionalProperties:
                      type: string
                    description: Optional. The labels to associate with this job.
                    type: object
                  pigConfig:
                    description: The config of pag job.
                    items:
                      properties:
                        continueOnFailure:
                          description: Whether to continue executing queries if a
                            query fails. The default value is false. Setting to true
                            can be useful when executing independent parallel queries.
                            Defaults to false.
                          type: boolean
                        jarFileUris:
                          description: HCFS URIs of jar files to add to the CLASSPATH
                            of the Pig Client and Hadoop MapReduce (MR) tasks. Can
                            contain Pig UDFs.
                          items:
                            type: string
                          type: array
                        loggingConfig:
                          description: The runtime logging config of the job
                          items:
                            properties:
                              driverLogLevels:
                                additionalProperties:
                                  type: string
                                description: 'Optional. The per-package log levels
                                  for the driver. This may include ''root'' package
                                  name to configure rootLogger. Examples: ''com.google
                                  = FATAL'', ''root = INFO'', ''org.apache = DEBUG''.'
                                type: object
                            required:
                            - driverLogLevels
                            type: object
                          type: array
                        properties:
                          additionalProperties:
                            type: string
                          description: A mapping of property names to values, used
                            to configure Pig. Properties that conflict with values
                            set by the Cloud Dataproc API may be overwritten. Can
                            include properties set in /etc/hadoop/conf/*-site.xml,
                            /etc/pig/conf/pig.properties, and classes in user code.
                          type: object
                        queryFileUri:
                          description: HCFS URI of file containing Hive script to
                            execute as the job. Conflicts with query_list
                          type: string
                        queryList:
                          description: The list of Hive queries or statements to execute
                            as part of the job. Conflicts with query_file_uri
                          items:
                            type: string
                          type: array
                        scriptVariables:
                          additionalProperties:
                            type: string
                          description: 'Mapping of query variable names to values
                            (equivalent to the Pig command: name=[value]).'
                          type: object
                      type: object
                    type: array
                  placement:
                    description: The config of job placement.
                    items:
                      properties:
                        clusterName:
                          description: The name of the cluster where the job will
                            be submitted
                          type: string
                      required:
                      - clusterName
                      type: object
                    type: array
                  project:
                    description: The project in which the cluster can be found and
                      jobs subsequently run against. If it is not provided, the provider
                      project is used.
                    type: string
                  pysparkConfig:
                    description: The config of pySpark job.
                    items:
                      properties:
                        archiveUris:
                          description: Optional. HCFS URIs of archives to be extracted
                            in the working directory of .jar, .tar, .tar.gz, .tgz,
                            and .zip
                          items:
                            type: string
                          type: array
                        args:
                          description: Optional. The arguments to pass to the driver.
                            Do not include arguments, such as --conf, that can be
                            set as job properties, since a collision may occur that
                            causes an incorrect job submission
                          items:
                            type: string
                          type: array
                        fileUris:
                          description: Optional. HCFS URIs of files to be copied to
                            the working directory of Python drivers and distributed
                            tasks. Useful for naively parallel tasks
                          items:
                            type: string
                          type: array
                        jarFileUris:
                          description: Optional. HCFS URIs of jar files to add to
                            the CLASSPATHs of the Python driver and tasks
                          items:
                            type: string
                          type: array
                        loggingConfig:
                          description: The runtime logging config of the job
                          items:
                            properties:
                              driverLogLevels:
                                additionalProperties:
                                  type: string
                                description: 'Optional. The per-package log levels
                                  for the driver. This may include ''root'' package
                                  name to configure rootLogger. Examples: ''com.google
                                  = FATAL'', ''root = INFO'', ''org.apache = DEBUG''.'
                                type: object
                            required:
                            - driverLogLevels
                            type: object
                          type: array
                        mainPythonFileUri:
                          description: Required. The HCFS URI of the main Python file
                            to use as the driver. Must be a .py file
                          type: string
                        properties:
                          additionalProperties:
                            type: string
                          description: Optional. A mapping of property names to values,
                            used to configure PySpark. Properties that conflict with
                            values set by the Cloud Dataproc API may be overwritten.
                            Can include properties set in /etc/spark/conf/spark-defaults.conf
                            and classes in user code
                          type: object
                        pythonFileUris:
                          description: 'Optional. HCFS file URIs of Python files to
                            pass to the PySpark framework. Supported file types: .py,
                            .egg, and .zip'
                          items:
                            type: string
                          type: array
                      required:
                      - mainPythonFileUri
                      type: object
                    type: array
                  reference:
                    description: The reference of the job
                    items:
                      properties:
                        jobId:
                          description: The job ID, which must be unique within the
                            project. The job ID is generated by the server upon job
                            submission or provided by the user as a means to perform
                            retries without creating duplicate jobs
                          type: string
                      type: object
                    type: array
                  region:
                    description: The Cloud Dataproc region. This essentially determines
                      which clusters are available for this job to be submitted to.
                      If not specified, defaults to global.
                    type: string
                  scheduling:
                    description: Optional. Job scheduling configuration.
                    items:
                      properties:
                        maxFailuresPerHour:
                          description: Maximum number of times per hour a driver may
                            be restarted as a result of driver exiting with non-zero
                            code before job is reported failed.
                          format: int64
                          type: integer
                        maxFailuresTotal:
                          description: Maximum number of times in total a driver may
                            be restarted as a result of driver exiting with non-zero
                            code before job is reported failed.
                          format: int64
                          type: integer
                      required:
                      - maxFailuresPerHour
                      - maxFailuresTotal
                      type: object
                    type: array
                  sparkConfig:
                    description: The config of the Spark job.
                    items:
                      properties:
                        archiveUris:
                          description: HCFS URIs of archives to be extracted in the
                            working directory of .jar, .tar, .tar.gz, .tgz, and .zip.
                          items:
                            type: string
                          type: array
                        args:
                          description: The arguments to pass to the driver.
                          items:
                            type: string
                          type: array
                        fileUris:
                          description: HCFS URIs of files to be copied to the working
                            directory of Spark drivers and distributed tasks. Useful
                            for naively parallel tasks.
                          items:
                            type: string
                          type: array
                        jarFileUris:
                          description: HCFS URIs of jar files to add to the CLASSPATHs
                            of the Spark driver and tasks.
                          items:
                            type: string
                          type: array
                        loggingConfig:
                          description: The runtime logging config of the job
                          items:
                            properties:
                              driverLogLevels:
                                additionalProperties:
                                  type: string
                                description: 'Optional. The per-package log levels
                                  for the driver. This may include ''root'' package
                                  name to configure rootLogger. Examples: ''com.google
                                  = FATAL'', ''root = INFO'', ''org.apache = DEBUG''.'
                                type: object
                            required:
                            - driverLogLevels
                            type: object
                          type: array
                        mainClass:
                          description: The class containing the main method of the
                            driver. Must be in a provided jar or jar that is already
                            on the classpath. Conflicts with main_jar_file_uri
                          type: string
                        mainJarFileUri:
                          description: The HCFS URI of jar file containing the driver
                            jar. Conflicts with main_class
                          type: string
                        properties:
                          additionalProperties:
                            type: string
                          description: A mapping of property names to values, used
                            to configure Spark. Properties that conflict with values
                            set by the Cloud Dataproc API may be overwritten. Can
                            include properties set in /etc/spark/conf/spark-defaults.conf
                            and classes in user code.
                          type: object
                      type: object
                    type: array
                  sparksqlConfig:
                    description: The config of SparkSql job
                    items:
                      properties:
                        jarFileUris:
                          description: HCFS URIs of jar files to be added to the Spark
                            CLASSPATH.
                          items:
                            type: string
                          type: array
                        loggingConfig:
                          description: The runtime logging config of the job
                          items:
                            properties:
                              driverLogLevels:
                                additionalProperties:
                                  type: string
                                description: 'Optional. The per-package log levels
                                  for the driver. This may include ''root'' package
                                  name to configure rootLogger. Examples: ''com.google
                                  = FATAL'', ''root = INFO'', ''org.apache = DEBUG''.'
                                type: object
                            required:
                            - driverLogLevels
                            type: object
                          type: array
                        properties:
                          additionalProperties:
                            type: string
                          description: A mapping of property names to values, used
                            to configure Spark SQL's SparkConf. Properties that conflict
                            with values set by the Cloud Dataproc API may be overwritten.
                          type: object
                        queryFileUri:
                          description: The HCFS URI of the script that contains SQL
                            queries. Conflicts with query_list
                          type: string
                        queryList:
                          description: The list of SQL queries or statements to execute
                            as part of the job. Conflicts with query_file_uri
                          items:
                            type: string
                          type: array
                        scriptVariables:
                          additionalProperties:
                            type: string
                          description: 'Mapping of query variable names to values
                            (equivalent to the Spark SQL command: SET name="value";).'
                          type: object
                      type: object
                    type: array
                required:
                - placement
                type: object
              providerConfigRef:
                default:
                  name: default
                description: ProviderConfigReference specifies how the provider that
                  will be used to create, observe, update, and delete this managed
                  resource should be configured.
                properties:
                  name:
                    description: Name of the referenced object.
                    type: string
                required:
                - name
                type: object
              providerRef:
                description: 'ProviderReference specifies the provider that will be
                  used to create, observe, update, and delete this managed resource.
                  Deprecated: Please use ProviderConfigReference, i.e. `providerConfigRef`'
                properties:
                  name:
                    description: Name of the referenced object.
                    type: string
                required:
                - name
                type: object
              writeConnectionSecretToRef:
                description: WriteConnectionSecretToReference specifies the namespace
                  and name of a Secret to which any connection details for this managed
                  resource should be written. Connection details frequently include
                  the endpoint, username, and password required to connect to the
                  managed resource.
                properties:
                  name:
                    description: Name of the secret.
                    type: string
                  namespace:
                    description: Namespace of the secret.
                    type: string
                required:
                - name
                - namespace
                type: object
            required:
            - forProvider
            type: object
          status:
            description: JobStatus defines the observed state of Job.
            properties:
              atProvider:
                properties:
                  driverControlsFilesUri:
                    type: string
                  driverOutputResourceUri:
                    type: string
                  status:
                    items:
                      properties:
                        details:
                          type: string
                        state:
                          type: string
                        stateStartTime:
                          type: string
                        substate:
                          type: string
                      type: object
                    type: array
                type: object
              conditions:
                description: Conditions of the resource.
                items:
                  description: A Condition that may apply to a resource.
                  properties:
                    lastTransitionTime:
                      description: LastTransitionTime is the last time this condition
                        transitioned from one status to another.
                      format: date-time
                      type: string
                    message:
                      description: A Message containing details about this condition's
                        last transition from one status to another, if any.
                      type: string
                    reason:
                      description: A Reason for this condition's last transition from
                        one status to another.
                      type: string
                    status:
                      description: Status of this condition; is it currently True,
                        False, or Unknown?
                      type: string
                    type:
                      description: Type of this condition. At most one of each condition
                        type may apply to a resource at any point in time.
                      type: string
                  required:
                  - lastTransitionTime
                  - reason
                  - status
                  - type
                  type: object
                type: array
            type: object
        required:
        - spec
        type: object
    served: true
    storage: true
    subresources:
      status: {}
status:
  acceptedNames:
    kind: ""
    plural: ""
  conditions: []
  storedVersions: []
